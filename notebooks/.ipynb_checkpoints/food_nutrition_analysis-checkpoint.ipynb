{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70eac546-fc7b-487a-95b6-a11f0ae86db6",
   "metadata": {},
   "source": [
    "# Food Nutrition Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c8f43-66ae-4c62-8f48-b006654e1a09",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Abstract](#Abstract)  \n",
    "    1.1 [Background](#Background)  \n",
    "\t1.2 [About](#About)\n",
    "2. [Procedures](#Procedures)  \n",
    "\t2.1 [OCR / Text-scan](#OCRandTextScan)  \n",
    "\t&nbsp; 2.1.1 [Identify Image Texts](#IdentifyImageTexts)  \n",
    "\t2.2 [Web Crawler for Data Collection](#WebCrawler)  \n",
    "    &nbsp; 2.2.1 [2.2.1 EWG Food Nutrition and Rating Information](#EWG)  \n",
    "    &nbsp; 2.2.2 [2.2.2 Costco Food Nutrition Facts Image Data](#Costco)  \n",
    "\t2.3 [Machine Learning Models](#MLModels)  \n",
    "\t&nbsp; 2.3.1 [Linear Regression](#LinearRegression)  \n",
    "\t&nbsp; 2.3.2 [Decision Tree Regressor](#DecisionTreeRegressor)  \n",
    "\t2.4 [Language Model](#LanguageModel)  \n",
    "\t2.5 [Recommendation System Algorithm](#RecommendationSystemAlgorithm)  \n",
    "\t2.6 [User-centered research](#UserCenteredResearch)  \n",
    "\t2.7 [Streamlit Dashboard](#Streamlit)\n",
    "4. [Conclusion](#Conclusion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0848c-d361-463a-a56d-995945f2ad4e",
   "metadata": {},
   "source": [
    "<a name=\"Abstract\"></a>\n",
    "## 1. Abstract \n",
    "<a name=\"Background\"></a>\n",
    "### 1.1 Background\n",
    "Smart food labeling provides a comprehensive breakdown of the nutritional information associated with the food product. The AI system scrutinizes the extracted text to ascertain the quantities of macronutrients (such as carbohydrates, proteins, and fats) and the total caloric content. This information is presented in a user-friendly format, allowing consumers to make informed decisions regarding their dietary choices.\n",
    "<a name=\"About\"></a>\n",
    "### 1.2 About\n",
    "A core aspect of smart food labeling is the meticulous analysis of the ingredients employed in food products. The AI system scrutinizes the extracted ingredients list, cross-referencing it against an extensive database of recognized ingredients. This analysis aids in identifying specific components that may be present in the product. This level of transparency empowers consumers to quickly ascertain whether a product aligns with their dietary needs or restrictions.\n",
    "Please view the code for detailed descriptions of what each function does, as such information is clearly recorded in respective docstrings. A detailed guide on how to run the project and how to setup the conda virtual environment is included in the README.md file.\n",
    "\n",
    "### 1.3 Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7f6e7c7f-ac32-4c62-9e1b-f73b090c34a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: easyocr in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: uuid in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (1.30)\n",
      "Requirement already satisfied: lxml in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (5.1.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: openai in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (1.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: torch in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (2.2.0)\n",
      "Requirement already satisfied: torchvision>=0.5 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (0.17.0)\n",
      "Requirement already satisfied: opencv-python-headless in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (4.9.0.80)\n",
      "Requirement already satisfied: scipy in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (1.10.1)\n",
      "Requirement already satisfied: Pillow in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (10.0.0)\n",
      "Requirement already satisfied: scikit-image in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (0.21.0)\n",
      "Requirement already satisfied: python-bidi in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (0.4.2)\n",
      "Requirement already satisfied: PyYAML in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (6.0)\n",
      "Requirement already satisfied: Shapely in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (2.0.2)\n",
      "Requirement already satisfied: pyclipper in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (1.3.0.post5)\n",
      "Requirement already satisfied: ninja in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from easyocr) (1.11.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from openai) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: certifi in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from torchvision>=0.5->easyocr) (2.31.0)\n",
      "Requirement already satisfied: filelock in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from torch->easyocr) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from torch->easyocr) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from torch->easyocr) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from torch->easyocr) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from torch->easyocr) (2024.2.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-image->easyocr) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-image->easyocr) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-image->easyocr) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-image->easyocr) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-image->easyocr) (0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from jinja2->torch->easyocr) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests->torchvision>=0.5->easyocr) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests->torchvision>=0.5->easyocr) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/simon198/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from sympy->torch->easyocr) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas easyocr uuid lxml scikit-learn openai streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184217cf-2cb0-40d0-89f7-7c3d12d9e079",
   "metadata": {},
   "source": [
    "### 1.4 Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b2713fb6-e469-4564-92f3-43499ecdfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import uuid\n",
    "import requests\n",
    "from lxml import etree\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import numpy as np\n",
    "from urllib.parse import urljoin, urlparse, urlsplit\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from openai import OpenAI\n",
    "import streamlit\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# this will prevent running the web crawling part of the notebook to save time and storage,\n",
    "# The execution of web crawling part is optional, it won't effect any other cell if you don't run it.\n",
    "web_crawling = False\n",
    "\n",
    "# this will prevent plotting user centered research chart\n",
    "# set it to true if you want to plot user centered research charts\n",
    "plot_user_center_chart = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad89228-9d08-4e2a-9443-25092a8546e7",
   "metadata": {},
   "source": [
    "<a name=\"Procedures\"></a>\n",
    "## 2. Procedures\n",
    "\n",
    "<a name=\"OCRandScanText\" ></a>\n",
    "### 2.1 OCR / Text-scan\n",
    "OCR stands for Optical Character Recognition. It's a technology that enables the conversion of different types of documents, such as scanned paper documents, PDF files, or images captured by a digital camera, into editable and searchable data. OCR software works by analyzing the patterns in the document images and recognizing the characters to convert them into machine-encoded text. \n",
    "In this project, we have used the easyOCR library to read text from the food nutrition label to obtain nutritional data, which becomes the basis of our analysis\n",
    "<a name=\"IdentifyImageTexts\"></a>\n",
    "#### 2.1.1 Identify Image Texts\n",
    "With easyOCR, we read the various text blocks within the food nutrition table image. This information is in the form of a list of strings. To demonstrate, we have included an image named “foodlabel.png” in the INPUTS folder, which is also the image we used for the demo during the presentation.\n",
    "In the coding process, we had to adjust a few parameters to get the clarity right, so that the library would group pieces of text together in the way we liked.\n",
    "The output is something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e66e2-6fd3-4252-89c7-54f9ea5fda1b",
   "metadata": {},
   "source": [
    "{\n",
    "    \"Category\" : \"meals\",\n",
    "    \"Calories\" : 240,\n",
    "    \"Total Fat\" : 4, \n",
    "    \"Saturated Fat\" : 1.5,\n",
    "    \"Trans Fat\" : 0,\n",
    "    \"Total Carbohydrate\" : 46,\n",
    "    \"Added Sugar\" : 2,\n",
    "    \"Protein\" : 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b5946e-9934-438b-a58a-ee36aafe0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file: str) -> list:\n",
    "    \"\"\"\n",
    "    Reads text from an image file using the easyOCR library.\n",
    "    ------------------\n",
    "    Parameters: \n",
    "    file (str): The path to the image file from which text needs to be extracted.\n",
    "    ------------------\n",
    "    Returns: \n",
    "    list: A list of strings where each string represents a detected line of text in the image.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    text = reader.readtext(file, detail = 0, text_threshold=0.7)\n",
    "    # print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b73fe3-c67b-456c-86f6-e08a258566e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_text(text: list, category: str) -> dict:\n",
    "    \"\"\"\n",
    "    Using the text output from read_text function, extract info and convert it into a \n",
    "    pandas df. Each time we run the read_text function, the output will become one row in \n",
    "    the final df. The idea is to get ~200 entries/rows (aka food items) in our database. \n",
    "    Maybe store the data in AWS or snowflake? Let me know what you guys think.\n",
    "    ------------------\n",
    "    Parameters: \n",
    "    text: text in str\n",
    "    category: category of food in str\n",
    "    ------------------\n",
    "    Returns: single-row table with columns as features in pd.DataFrame\n",
    "    \"\"\"\n",
    "    \"\"\" \n",
    "    features: \n",
    "    Calories from added sugar/total calories\n",
    "    Calories from fat/total calories\n",
    "    Calories from protein/total calories\n",
    "    Calories from carbs/total calories\n",
    "    Calories from saturated fat/total calories\n",
    "    Calories from trans fat/total calories\n",
    "    more to be added\n",
    "    \"\"\"\n",
    "    nutrition_map = {}\n",
    "    nutrition_map['Category'] = category\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == 'Calories':\n",
    "            nutrition_map['Calories'] = int(text[i+1])\n",
    "            continue\n",
    "        if 'Total Fat' in text[i]:\n",
    "            nutrition_map['Total Fat'] = int(text[i].split(' ')[-1][:(len(text[i]) - 1)][:-1])\n",
    "            continue\n",
    "        if 'Saturated Fat' in text[i]:\n",
    "            nutrition_map['Saturated Fat'] = float(text[i].split(' ')[-1][:(len(text[i]) - 1)][:-1])\n",
    "            continue\n",
    "        if 'Trans Fat' in text[i]:\n",
    "            tmp = text[i].split(' ')[-1][:(len(text[i]) - 1)][:-1]\n",
    "            tmp = 0 if tmp == 'O' else int(tmp)\n",
    "            nutrition_map['Trans Fat'] = tmp\n",
    "            continue\n",
    "        if 'Total Carbohydrate' in text[i]:\n",
    "            nutrition_map['Total Carbohydrate'] = int(text[i].split(' ')[-1][:(len(text[i]) - 1)][:-1])\n",
    "            continue\n",
    "        if 'Added Sugar' in text[i]:\n",
    "            nutrition_map['Added Sugar'] = int(text[i].split(' ')[-3][:(len(text[i]) - 3)][:-1])\n",
    "            continue\n",
    "        if 'Protein' in text[i]:\n",
    "            nutrition_map['Protein'] = int(text[i+1][:-1])\n",
    "            continue\n",
    "    return nutrition_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09771219-a1d5-456b-802f-ff4b021e2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_info_to_df(d: dict)-> pd.DataFrame:\n",
    "    nutrition_map = {}\n",
    "    nutrition_map['category'] = d['Category']\n",
    "    # Calories from Added Sugar vs Total Calories\n",
    "    nutrition_map['suga_to_total'] = round(d['Added Sugar'] * 4 / d['Calories'], 4)\n",
    "    # Calories from Fat vs Total Calories\n",
    "    nutrition_map['fat_to_total'] = round(d['Total Fat'] * 9 / d['Calories'], 4)\n",
    "    # Calories from Protein vs Total Calories\n",
    "    nutrition_map['pro_to_total'] = round(d['Protein'] * 4 / d['Calories'], 4)\n",
    "    # Calories from Carbohydrates vs Total Calories\n",
    "    nutrition_map['carb_to_total'] = round(d['Total Carbohydrate'] * 4 / d['Calories'], 4)\n",
    "    # Calories from Saturated Fat vs Total Calories\n",
    "    nutrition_map['satu_to_total'] = round(d['Saturated Fat'] * 9 / d['Calories'], 4)\n",
    "    # Calories from Trans Fat vs Total Calories\n",
    "    nutrition_map['tran_to_total'] = round(d['Trans Fat'] * 9 / d['Calories'], 4)\n",
    "    res = pd.DataFrame(nutrition_map, index=[str(uuid.uuid4())])\n",
    "    print(res)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0db97f3-dbc8-41e8-b4a3-1ebc2cd840d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     category  suga_to_total  fat_to_total  \\\n",
      "e7e4696c-e7e4-438e-8e88-399977476706    food1         0.0333          0.15   \n",
      "\n",
      "                                      pro_to_total  carb_to_total  \\\n",
      "e7e4696c-e7e4-438e-8e88-399977476706        0.1833         0.7667   \n",
      "\n",
      "                                      satu_to_total  tran_to_total  \n",
      "e7e4696c-e7e4-438e-8e88-399977476706         0.0563            0.0  \n"
     ]
    }
   ],
   "source": [
    "text = read_text('../INPUTS/foodlabel.png')\n",
    "food_info_df = convert_info_to_df(extract_info_from_text(text, 'food1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe14ee-7e99-412b-afed-4e5770099f83",
   "metadata": {},
   "source": [
    "<a name=\"WebCrawler\"></a>\n",
    "### 2.2 Web Crawler for Data Collection\n",
    "<a name=\"EWG\"></a>\n",
    "#### 2.2.1 EWG Food Nutrition and Rating Information\n",
    "##### 2.2.1.1 Overview\n",
    "In this section, we focus on the web crawler developed to collect food impact information and ratings from the Environmental Working Group (EWG) website. The purpose of this crawler is to gather data that serves as a valuable sample for training the subsequent machine learning models.\n",
    "##### 2.2.1.2 Implementation\n",
    "The crawler navigates through the EWG food scores webpage, specifically targeting food products, their details, and associated nutritional information. The following steps outline the process:\n",
    "1. Page Navigation:\n",
    "The crawler systematically explores the EWG food scores pages, scraping information for a predetermined number of products per page.\n",
    "2. Data Extraction:\n",
    "For each product, the crawler extracts details such as the product name, link to the product page, size information, and nutritional content.\n",
    "3. Additional Information:\n",
    "The crawler also captures specific details like the caloric content and the product's overall score, providing a comprehensive dataset for subsequent analysis.\n",
    "2.2.1.3 Challenges and Solutions\n",
    "1. Dynamic Content: The EWG website employs dynamic loading of content, requiring careful handling to ensure complete data retrieval.\n",
    "2. Parsing Complexity: The information on the website is nested and requires careful parsing, which was successfully achieved using XPath expressions.\n",
    "3. Rate Limiting: The crawler includes a mechanism to handle HTTP 429 status codes, indicative of exceeding the allowed number of requests within a specified time frame. When such a status code is encountered, the program gracefully pauses execution for a predetermined duration before resuming the crawling process.\n",
    "4. Robots.txt Compliance: In adherence to web etiquette and legal considerations, both crawlers strictly adhere to the rules defined in the `robots.txt` files of the respective websites. This ensures that the crawling activity is within the bounds defined by the website administrators.\n",
    "##### 2.2.1.4 Output\n",
    "The gathered data is stored in a structured format, with each entry containing essential details about a food product. The resulting dataset serves as a valuable resource for training machine learning models in subsequent stages of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc78b2-55f4-476c-be73-c316296b6d5d",
   "metadata": {},
   "source": [
    "##### Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64fb9e4e-34c6-4189-af36-ce316e0ef00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(url):\n",
    "    \"\"\"\n",
    "    Makes a GET request to the specified URL with a user-defined header to mimic a web browser request.\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): Target URL from which to fetch the content.\n",
    "    \n",
    "    Returns:\n",
    "    - Response object from requests library.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.57'}\n",
    "    resp = requests.get(url, headers=headers, timeout=15)\n",
    "    return resp\n",
    "\n",
    "def get_food_label(page_number):\n",
    "    \"\"\"\n",
    "    Fetches food product labels from the EWG website for a specific page number.\n",
    "    \n",
    "    Parameters:\n",
    "    - page_number (int): The specific page number to scrape.\n",
    "    \n",
    "    Returns:\n",
    "    - List of anchor elements (<a> tags) containing product information.\n",
    "    \"\"\"\n",
    "    page_url = f'https://www.ewg.org/foodscores/products/?category_group=&page={page_number}&per_page=48&type=products'\n",
    "    resp = get_response(page_url)\n",
    "    a_labels = etree.HTML(resp.text).xpath('//div[@class=\"ind_result_text fleft\"]/a')\n",
    "    return a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1ac7b1c-4e27-46d8-827b-797585a301c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food_info(a):\n",
    "    \"\"\"\n",
    "    Extracts detailed food product information from its detail page.\n",
    "    \n",
    "    Parameters:\n",
    "    - a (element): The anchor element pointing to the product's detail page.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing nutritional facts and other details of the product.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nutrition_fact = {}\n",
    "        detail_url = urljoin('https://www.ewg.org/foodscores/products/', a.get('href'))\n",
    "        resp_text = get_response(detail_url).text\n",
    "        resp = etree.HTML(resp_text)\n",
    "        name = resp.xpath('//h1[@class=\"truncate_title_specific_product_page\"]')[0].text\n",
    "        clr = resp.xpath('//div[@id=\"nut_calories_value\"]')[0].text\n",
    "        nutrition_fact['name'] = name\n",
    "        nutrition_fact['link'] = detail_url\n",
    "        size_opt = resp.xpath('//option[@selected]')\n",
    "        nutrition_fact['size'] = size_opt[0].text if size_opt else None\n",
    "        nutrition_fact['Calories'] = clr\n",
    "        pattern = r'score_(\\d+)_(\\d+)'\n",
    "        matches = re.findall(pattern, resp_text)\n",
    "        score = '.'.join(matches[0]) if matches else None\n",
    "        for i in resp.xpath('//span[@class=\"normal_title\" or @class=\"med_title\"]'):\n",
    "            name_n_value = i.xpath('string(./..)').strip()\n",
    "            element_name = i.xpath('string(.)').strip()\n",
    "            element_kv = name_n_value.split(element_name)\n",
    "            element_value_detail = element_kv[-1].strip() if len(element_kv) > 1 else ''\n",
    "            nutrition_fact[element_name] = element_value_detail\n",
    "        return nutrition_fact\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "578e6918-0137-49c3-8321-301eb1c4d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if web_crawling:\n",
    "    # Main driver code: Iterates through EWG food score pages and aggregates data into a CSV file.\n",
    "    ewg_data_list = []\n",
    "    # UID counter for unique identification of entries\n",
    "    ct = 20000000\n",
    "    start_url = \"https://www.ewg.org/foodscores/products/?category_group=&page=1&per_page=48&type=products\"\n",
    "    # Determine the total number of pages to crawl\n",
    "    pages = int(etree.HTML(get_response(start_url).text).xpath('//a[@aria-label]')[-1].text)\n",
    "    \n",
    "    pages = 50 if pages > 50 else pages\n",
    "    \n",
    "    for page_number in range(1, pages+1):\n",
    "        food_labels = get_food_label(page_number)\n",
    "        for a in food_labels:\n",
    "            nutrition_fact = get_food_info(a)\n",
    "            if nutrition_fact:\n",
    "                nutrition_fact['uid'] = ct\n",
    "                ct += 1\n",
    "                ewg_data_list.append(nutrition_fact)\n",
    "            else:\n",
    "                break\n",
    "    # Convert aggregated data into a pandas DataFrame\n",
    "    ewg_data_df = pd.DataFrame(ewg_data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d5f61-1084-4cbf-9032-7d47e78bfdc7",
   "metadata": {},
   "source": [
    "##### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83e42ec0-6452-407b-99ca-41eeca759276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_convert(value):\n",
    "    \"\"\"\n",
    "    Parses nutritional values from string to float, handling different units and converting as necessary.\n",
    "    \n",
    "    Parameters:\n",
    "    - value (str): The string containing the nutritional value and unit.\n",
    "    \n",
    "    Returns:\n",
    "    - The converted value as float, or the original value if conversion isn't applicable.\n",
    "    \"\"\"\n",
    "    match = re.match(r'<?(\\d+(\\.\\d+)?)\\s*(\\D+)', str(value).replace(' ', ''))\n",
    "    if match:\n",
    "        number, _, unit = match.groups()\n",
    "        unit_mapping = {\n",
    "            'g': 1,\n",
    "            'mg': 0.001,\n",
    "            'q': 1,\n",
    "            '[g, g]': 1,\n",
    "            'g*': 1,\n",
    "            'g,': 1,\n",
    "            '%': 1,\n",
    "        }\n",
    "        if unit.lower() in unit_mapping:\n",
    "            return float(number) * unit_mapping[unit.lower()]\n",
    "    elif str(value).replace('<', '').isdigit():\n",
    "        return str(value).replace('<', '')\n",
    "    else:\n",
    "        return \"Invalid Data\"\n",
    "        \n",
    "\n",
    "def rename(df):\n",
    "    \"\"\"\n",
    "    Renames columns of the DataFrame to standardize names for further processing and ensures all expected columns exist.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame with original column names.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: The DataFrame with columns renamed and standardized.\n",
    "    \"\"\"\n",
    "    # Define a mapping of existing column names in df to their new standardized names\n",
    "    rename_mapping = {\n",
    "        'Total Carbs': 'Total Carbohydrate',\n",
    "        'Added Sugar Ingredients:': 'Added Sugar',\n",
    "        # Add any additional column renaming rules here\n",
    "    }\n",
    "    \n",
    "    # Rename the columns based on the mapping\n",
    "    # 'errors=ignore' ensures that non-existing columns in the mapping do not cause an error\n",
    "    df.rename(columns=rename_mapping, inplace=True, errors='ignore')\n",
    "    \n",
    "    # List of all expected column names after renaming\n",
    "    expected_columns = [\n",
    "        'uid', 'name', 'score', 'Calories', 'Total Fat', 'Total Carbohydrate',\n",
    "        'Protein', 'Saturated Fat', 'Trans Fat', 'Cholesterol', 'Sodium',\n",
    "        'Added Sugar', 'Dietary Fiber', 'Sugars'\n",
    "    ]\n",
    "    \n",
    "    # Add any missing expected columns as empty (None values)\n",
    "    for column in expected_columns:\n",
    "        if column not in df.columns:\n",
    "            df[column] = 0  # Adds the column with None as default value for all rows\n",
    "    \n",
    "    # Optionally, reorder DataFrame columns to match the expected_columns order\n",
    "    df = df[expected_columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def del_invalid(df):\n",
    "    \"\"\"\n",
    "    Removes rows with invalid data from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame to clean.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with invalid rows removed.\n",
    "    \"\"\"\n",
    "    all_columns = df.columns\n",
    "    for column in all_columns:\n",
    "        df = df[~(df[column] == 'Invalid Data')]\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_convert_types(df, strategy='fill', fill_value=0):\n",
    "    \"\"\"\n",
    "    Cleans specified columns in a DataFrame by replacing non-numeric values with NaN,\n",
    "    converts them to float, and handles missing values according to the specified strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame to clean and convert.\n",
    "    - strategy (str): Strategy to handle missing values ('fill' to fill with a value, 'drop' to drop rows with NaN).\n",
    "    - fill_value (float or dict): The value to fill NaNs with if strategy is 'fill'. Can be a single value or a dict specifying per-column fill values.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: The cleaned and type-converted DataFrame.\n",
    "    \"\"\"\n",
    "    columns_to_convert = [\n",
    "        'score', 'Calories', 'Total Fat', 'Total Carbohydrate', 'Protein', \n",
    "        'Saturated Fat', 'Trans Fat', 'Cholesterol', 'Sodium', 'Dietary Fiber', 'Sugars'\n",
    "    ]\n",
    "    \n",
    "    # Replace known non-numeric values with NaN for specified columns\n",
    "    df[columns_to_convert] = df[columns_to_convert].replace(['n/a', 'None', '--'], np.nan)\n",
    "    \n",
    "    # Convert columns to numeric, coercing errors to NaN\n",
    "    for column in columns_to_convert:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    \n",
    "    # Handle missing values based on the specified strategy\n",
    "    if strategy == 'fill':\n",
    "        df.fillna(fill_value, inplace=True)\n",
    "    elif strategy == 'drop':\n",
    "        df.dropna(subset=columns_to_convert, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b920ab40-a5a0-4e60-a18e-324b92f6ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if web_crawling:\n",
    "    ewg_data_df = rename(ewg_data_df)\n",
    "    ewg_data_df.fillna('0', inplace=True)\n",
    "    ewg_data_df[['Total Fat', 'Total Carbohydrate', 'Protein', 'Saturated Fat', 'Trans Fat', 'Cholesterol', 'Sodium',\n",
    "        'Dietary Fiber', 'Sugars']] = ewg_data_df[\n",
    "        ['Total Fat', 'Total Carbohydrate', 'Protein', 'Saturated Fat', 'Trans Fat', 'Cholesterol', 'Sodium',\n",
    "         'Dietary Fiber', 'Sugars']].applymap(parse_and_convert)\n",
    "    \n",
    "    ewg_data_df = del_invalid(ewg_data_df)\n",
    "    \n",
    "    ewg_data_df = clean_and_convert_types(ewg_data_df)\n",
    "    print(ewg_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b5ca0-ba6e-4825-a43e-7dfb379c4f62",
   "metadata": {},
   "source": [
    "<a name=\"Costco\"></a>\n",
    "#### 2.2.2 Costco Food Nutrition Facts Image Data\n",
    "##### 2.2.2.1 Overview\n",
    "This section focuses on the second web crawler, designed to collect nutrition facts information from images on the Costco website. The goal is to utilize OCR for extracting relevant details from food labels and enriching the dataset.\n",
    "##### 2.2.2.2 Implementation\n",
    "1. Category Selection:\n",
    "The crawler systematically navigates through specified categories on the Costco website to target relevant food products.\n",
    "2. Image Retrieval:\n",
    "For each product, the crawler identifies the product name, link to the product page, profile ID, and image ID.\n",
    "3. Confirmation of Relevant Image:\n",
    "Another challenge involves confirming which image on the webpage corresponds to the nutritional content of the product. This required careful analysis of the webpage structure and content, ensuring that the correct image associated with nutritional information is selected.\n",
    "4. Image Download:\n",
    "The crawler utilizes the identified IDs to download images containing nutritional information, focusing on the textual content related to nutritional facts.\n",
    "##### 2.2.2.3 Challenges and Solutions\n",
    "1. Image Retrieval: Extracting the correct image details from the Costco website required careful analysis of the web page structure.\n",
    "2. Image Download: Handling different image formats and ensuring successful downloads were key considerations addressed during implementation.\n",
    "3. Rate Limiting: To prevent potential issues, a time delay of 1 second was introduced between requests to the server.\n",
    "4. Robots.txt Compliance: As with the EWG crawler, the Costco crawler respects the guidelines set forth in the `robots.txt` file, ensuring ethical and legal use of web scraping techniques.\n",
    "##### 2.2.2.4 Output\n",
    "The collected data is stored in a CSV file containing unique identifiers, product names, and links. This dataset serves as a valuable resource for further analysis and integration into the overall project framework. Images associated with nutritional content are stored locally using the format {uid}.jpg in the same directory as the scripts, ensuring a systematic and easily accessible storage approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8e2ee-7f70-4688-b674-b2d7daa0f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of a given URL using a specified User-Agent in the headers.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL from which to fetch content.\n",
    "\n",
    "    Returns:\n",
    "    - Response object: The response from the requests.get() call.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.57'}\n",
    "    resp = requests.get(url, headers=headers, timeout=15)\n",
    "    return resp\n",
    "\n",
    "\n",
    "def get_pages(url):\n",
    "    \"\"\"\n",
    "    Determines the total number of pages for a website section based on the total number of products.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the website section to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - page_total (int): The total number of pages calculated from the total products.\n",
    "    \"\"\"\n",
    "    resp = get_response(url)\n",
    "    page = etree.HTML(resp.text).xpath('//span[@automation-id=\"totalProductsOutputText\"]')[0].text\n",
    "    page_total = ceil(int(page.split('of')[-1].strip()) / 24)\n",
    "    return page_total\n",
    "\n",
    "\n",
    "def get_detail_labels(page_url):\n",
    "    \"\"\"\n",
    "    Extracts the detail labels (links to product detail pages) from a given page URL.\n",
    "\n",
    "    Parameters:\n",
    "    - page_url (str): The URL of the page from which to extract product details links.\n",
    "\n",
    "    Returns:\n",
    "    - detail_labels (list): A list of elements pointing to product detail pages.\n",
    "    \"\"\"\n",
    "    resp = get_response(page_url)\n",
    "    detail_labels = etree.HTML(resp.text).xpath(\n",
    "        '//div[@automation-id=\"productList\"]//a[contains(@automation-id,\"productDescriptionLink\")]')\n",
    "    return detail_labels\n",
    "\n",
    "\n",
    "def parse_detail_page(label):\n",
    "    \"\"\"\n",
    "    Parses the product detail page for specific product information and image IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - label (element): An element containing the product detail link and name.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing product name, detail URL, profile ID, and image ID. If no match is found, returns None for all.\n",
    "    \"\"\"\n",
    "    time.sleep(1) # Delays the request to avoid overwhelming the server\n",
    "    pd_name = label.text.strip()\n",
    "    detail_url = label.get('href')\n",
    "    detail_page = etree.HTML(get_response(detail_url).text)\n",
    "    img_src = detail_page.xpath('//meta[@property=\"og:image\"]')[0].get('content')\n",
    "    pattern = r\"profileId=(\\d+)&imageId=(\\d+-?\\d*)\"\n",
    "    match = re.search(pattern, img_src)\n",
    "    if match:\n",
    "        profile_id = match.group(1)\n",
    "        image_id = match.group(2)\n",
    "        return pd_name, detail_url, profile_id, image_id\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def download_images(profile_id, image_id):\n",
    "    \"\"\"\n",
    "    Downloads images based on the profile ID and image ID obtained from the product's detail page.\n",
    "\n",
    "    Parameters:\n",
    "    - profile_id (str): The profile ID for the image.\n",
    "    - image_id (str): The specific image ID to download.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the image was successfully downloaded, False otherwise.\n",
    "    \"\"\"\n",
    "    img_base = f'https://richmedia.ca-richimage.com/ViewerDelivery/productXmlService?profileid={profile_id}&itemid={image_id}&viewerid=1068&callback='\n",
    "    img_page = get_response(img_base).text.strip('()')\n",
    "    try:\n",
    "        img_json = json.loads(img_page)\n",
    "        imgs = img_json['product']['views']\n",
    "        for img in imgs:\n",
    "            if 'nf' in img.get('@name'):\n",
    "                nf_url = img['swatches'][0]['images'][0]['@path']\n",
    "                img_file = get_response(nf_url).content\n",
    "                with open(f'{uid}.jpg', 'wb') as f:\n",
    "                    f.write(img_file)\n",
    "                return True\n",
    "            else:\n",
    "                continue\n",
    "        return False\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f6be0091-2530-458d-8bfc-80ee0f084095",
   "metadata": {},
   "outputs": [],
   "source": [
    "if web_crawling:\n",
    "    costco = 'https://www.costco.com/grocery-household.html'\n",
    "    start_list = [\n",
    "        '/snacks.html',\n",
    "        \"/coffee-sweeteners.html\",\n",
    "        \"/candy.html\",\n",
    "        \"/pantry.html\",\n",
    "        \"/breakfast.html\",\n",
    "        \"/beverages.html\",\n",
    "        \"/emergency-kits-supplies.html\",\n",
    "        \"/organic-groceries.html\",\n",
    "        \"/cheese.html\",\n",
    "        \"/deli.html\",\n",
    "        \"/cakes-cookies.html\",\n",
    "    ]\n",
    "    \n",
    "    uid = 1000000\n",
    "    \n",
    "    with open('costco_food_img.csv', 'w') as f:\n",
    "        f.write('uid\\tname\\tlink\\n')\n",
    "    \n",
    "    for start_url in start_list:\n",
    "        page_total = get_pages(urljoin(costco, start_url))\n",
    "        for i in range(0, page_total):\n",
    "            page_url = f'https://www.costco.com{start_url}?currentPage={i + 1}&pageSize=24'\n",
    "            detail_labels = get_detail_labels(page_url)\n",
    "            for label in detail_labels:\n",
    "                pd_name, detail_url, profile_id, image_id = parse_detail_page(label)\n",
    "                uid += 1\n",
    "                if profile_id and image_id:\n",
    "                    down_img = download_images(profile_id, image_id)\n",
    "                    if down_img:\n",
    "                        with open('costco_food_img.csv', 'a') as f:\n",
    "                            f.write(f'{uid}\\t{pd_name}\\t{detail_url}\\n')\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c2798-b146-432b-a18a-aa80c7a1ce3f",
   "metadata": {},
   "source": [
    "<a name=\"MLModels\"></a>\n",
    "### 2.3 Machine Learning Models\n",
    "#### 2.3.1 Features\n",
    "First, we chose the features for our models, which include calories from sugar to total calories, calories from fat to total calories, calories from protein to total calories, calories from carbs to total calories, calories from saturated fat to total calories, calories from trans fat to total calories. The reason we chose ratio instead of actual calorie counts is so that we can compare the data of solid foods and drinks, since most beverages tend to be less calorie-dense, while it is still possible for them to have a bad calorie ratio and poor nutrition value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83839330-549a-4277-97cc-0571ef63f67f",
   "metadata": {},
   "source": [
    "<a name=\"LinearRegression\"></a>\n",
    "#### 2.3.2 Linear Regression\n",
    "Linear regression is a parametric algorithm, meaning it makes certain assumptions about the underlying data distribution. It assumes a linear relationship between the input features and the output variable. The model is represented by the equation of a straight line (in the case of simple linear regression) or a hyperplane (in the case of multiple linear regression). The objective of linear regression is to find the coefficients of the linear equation that minimizes the sum of squared differences between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "158b7fc0-1885-4732-9cb0-fca8566e626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the data into training and testing sets.\n",
    "        \"\"\"\n",
    "        X = self.data[['suga_to_total', 'fat_to_total', 'pro_to_total', 'carb_to_total', 'satu_to_total', 'tran_to_total']]\n",
    "        y = self.data['Score']\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the Linear Regression model using the training data.\n",
    "        \"\"\"\n",
    "        self.model = LinearRegression()\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model's performance on the test set.\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        mse = mean_squared_error(self.y_test, y_pred)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        return mse, r2\n",
    "\n",
    "    def get_coefficients(self):\n",
    "        \"\"\"\n",
    "        Retrieves the model's coefficients and intercept.\n",
    "        \"\"\"\n",
    "        coefficients = dict(zip(['suga_to_total', 'fat_to_total', 'pro_to_total', 'carb_to_total', 'satu_to_total', 'tran_to_total'], self.model.coef_))\n",
    "        coefficients['Intercept'] = self.model.intercept_\n",
    "        return coefficients\n",
    "\n",
    "    def predict_new_data(self, new_data):\n",
    "        \"\"\"\n",
    "        Predicts the score for new data points based on the trained model.\n",
    "        \"\"\"\n",
    "        features = [round(new_data[feature] * conversion_factor / new_data['Calories'], 4) \n",
    "                    for feature, conversion_factor in zip(['Added Sugar', 'Total Fat', 'Protein', 'Total Carbohydrate', 'Saturated Fat', 'Trans Fat'], \n",
    "                                                          [4, 9, 4, 4, 9, 9])]\n",
    "        predicted_score = self.model.predict([features])[0]\n",
    "        return predicted_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d42cf00b-d896-444e-9145-3674abaee868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_info_to_df_databse_lr(df: pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for Linear Regression Model training by calculating nutrient ratios.\n",
    "    \"\"\"\n",
    "    columns = ['category', 'suga_to_total', 'fat_to_total', 'pro_to_total', 'carb_to_total', 'satu_to_total', 'tran_to_total', 'Score', 'category']  # Replace these with your column names\n",
    "    nutrition = pd.DataFrame(columns=columns)\n",
    "    nutrition['Name'] = df['Name']\n",
    "    nutrition['Score'] = df['Score']\n",
    "    nutrition['category'] = df['Category']\n",
    "    # Calories from Added Sugar vs Total Calories\n",
    "    nutrition['suga_to_total'] = round(df['Added Sugar'] * 4 / df['Calories'], 4)\n",
    "    # Calories from Fat vs Total Calories\n",
    "    nutrition['fat_to_total'] = round(df['Total Fat'] * 9 / df['Calories'], 4)\n",
    "    # Calories from Protein vs Total Calories\n",
    "    nutrition['pro_to_total'] = round(df['Protein'] * 4 / df['Calories'], 4)\n",
    "    # Calories from Carbohydrates vs Total Calories\n",
    "    nutrition['carb_to_total'] = round(df['Total Carbohydrate'] * 4 / df['Calories'], 4)\n",
    "    # Calories from Saturated Fat vs Total Calories\n",
    "    nutrition['satu_to_total'] = round(df['Saturated Fat'] * 9 / df['Calories'], 4)\n",
    "    # Calories from Trans Fat vs Total Calories\n",
    "    nutrition['tran_to_total'] = round(df['Trans Fat'] * 9 / df['Calories'], 4)\n",
    "    return nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2096daea-69c3-4630-8a3d-8f312dbf07aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.3831290276348995\n",
      "R-squared (R2) Score: 0.10617462942274791\n",
      "Model Coefficients:\n",
      "suga_to_total: 2.4908297693439345\n",
      "fat_to_total: 1.1938017876213083\n",
      "pro_to_total: -3.230754564358422\n",
      "carb_to_total: -0.9041395636852857\n",
      "satu_to_total: 2.3359050299067583\n",
      "tran_to_total: -6.145781487898252\n",
      "Intercept: 5.164429146560749\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../INPUTS/data_for_model_training.csv')\n",
    "\n",
    "# Preprocess data\n",
    "df_prepared = convert_info_to_df_databse_lr(df)\n",
    "df_prepared = df_prepared.dropna()  # Ensure there are no NaN values\n",
    "\n",
    "# Initialize and train model\n",
    "model = LinearRegressionModel(df_prepared)\n",
    "model.split_data(test_size=0.2, random_state=42)\n",
    "model.train_model()\n",
    "\n",
    "# Evaluate model\n",
    "mse, r2 = model.evaluate_model()\n",
    "coefficients = model.get_coefficients()\n",
    "\n",
    "# Display results\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared (R2) Score: {r2}\")\n",
    "print(\"Model Coefficients:\")\n",
    "for feature, coef in coefficients.items():\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "# Predictions can be made with model.predict_new_data(new_data) where new_data is a dictionary of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d89ae-2b81-4e04-aa23-de1b236cc667",
   "metadata": {},
   "source": [
    "<a name=\"decisionTreeRegressor\"></a>\n",
    "#### 2.3.3 Decision Tree Regressor\n",
    "Decision trees are non-parametric algorithms that do not make strong assumptions about the underlying data distribution. A decision tree is a hierarchical tree-like structure where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents the predicted value. The goal of a decision tree regressor is to recursively split the data into subsets based on the most informative features and assign a constant value to each leaf node, minimizing the variance within each leaf. The decision tree is built by recursively splitting the data based on features, with each split chosen to maximize the reduction in variance or mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0600344e-4193-4fab-8e83-b53fd7948177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressorModel:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the DecisionTreeRegressorModel with the provided data.\n",
    "        \n",
    "        Parameters:\n",
    "        - data (DataFrame): The dataset containing features and target variable for model training.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and testing sets.\n",
    "        \n",
    "        Parameters:\n",
    "        - test_size (float): The proportion of the dataset to include in the test split.\n",
    "        - random_state (int, optional): Controls the shuffling applied to the data before applying the split.\n",
    "        \"\"\"\n",
    "        X = self.data[['suga_to_total', 'fat_to_total', 'pro_to_total', 'carb_to_total', 'satu_to_total', 'tran_to_total']]\n",
    "        y = self.data['Score']\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the Decision Tree Regressor model using the training dataset.\n",
    "        \"\"\"\n",
    "        self.model = DecisionTreeRegressor(random_state=42)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model's performance on the test dataset.\n",
    "        \n",
    "        Returns:\n",
    "        - mse (float): Mean Squared Error of the model predictions.\n",
    "        - r2 (float): R-squared score of the model.\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        mse = mean_squared_error(self.y_test, y_pred)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        return mse, r2\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Retrieves the feature importance determined by the model.\n",
    "        \n",
    "        Returns:\n",
    "        - Array of feature importance scores.\n",
    "        \"\"\"\n",
    "        return self.model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "87b21195-4c7e-42de-b71d-3cc83f76dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_info_to_df_database_dt(df: pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the DataFrame for the Decision Tree Regressor Model by calculating nutrient ratios.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The original DataFrame with nutritional information.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Prepared DataFrame with calculated features for model training.\n",
    "    \"\"\"\n",
    "    columns = ['Name', 'suga_to_total', 'fat_to_total', 'pro_to_total', 'carb_to_total', 'satu_to_total', 'tran_to_total', 'Score', 'category']  # Replace these with your column names\n",
    "    nutrition = pd.DataFrame(columns=columns)\n",
    "    nutrition['Name'] = df['Name']\n",
    "    nutrition['Score'] = df['Score']\n",
    "    nutrition['category'] = df['Category']\n",
    "    # Calories from Added Sugar vs Total Calories\n",
    "    nutrition['suga_to_total'] = round(df['Added Sugar'] * 4 / df['Calories'], 4)\n",
    "    # Calories from Fat vs Total Calories\n",
    "    nutrition['fat_to_total'] = round(df['Total Fat'] * 9 / df['Calories'], 4)\n",
    "    # Calories from Protein vs Total Calories\n",
    "    nutrition['pro_to_total'] = round(df['Protein'] * 4 / df['Calories'], 4)\n",
    "    # Calories from Carbohydrates vs Total Calories\n",
    "    nutrition['carb_to_total'] = round(df['Total Carbohydrate'] * 4 / df['Calories'], 4)\n",
    "    # Calories from Saturated Fat vs Total Calories\n",
    "    nutrition['satu_to_total'] = round(df['Saturated Fat'] * 9 / df['Calories'], 4)\n",
    "    # Calories from Trans Fat vs Total Calories\n",
    "    nutrition['tran_to_total'] = round(df['Trans Fat'] * 9 / df['Calories'], 4)\n",
    "    return nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "70a23136-65c6-40e6-97e0-55fc97c469f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.332\n",
      "R-squared (R2) Score: -0.14451783355350067\n",
      "Feature Importance:\n",
      "suga_to_total: 0.2880020266050098\n",
      "fat_to_total: 0.1492217527815166\n",
      "pro_to_total: 0.2706288488089343\n",
      "carb_to_total: 0.13437789144090984\n",
      "satu_to_total: 0.15776948036362962\n",
      "tran_to_total: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../INPUTS/data_for_model_training.csv')\n",
    "# Prepare the dataset\n",
    "df_prepared = convert_info_to_df_database_dt(df)\n",
    "df_prepared = df_prepared.dropna()  # Ensure there are no NaN values\n",
    "\n",
    "# Initialize, train, and evaluate the model\n",
    "model = DecisionTreeRegressorModel(df_prepared)\n",
    "model.split_data(test_size=0.2, random_state=42)\n",
    "model.train_model()\n",
    "mse, r2 = model.evaluate_model()\n",
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "# Display evaluation results and feature importance\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared (R2) Score: {r2}\")\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in zip(df_prepared.columns[1:7], feature_importance):  # Adjust column slicing as needed\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd82422-ba25-4585-9401-9210609a833f",
   "metadata": {},
   "source": [
    "#### 2.3.4 Implementation\n",
    "We compared the Mean Squared Error and R squared value and concluded that the linear regression model is the more suitable model, so we used it to estimate the score for the nutrition label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a05e83-5f5b-4c60-8e03-e8ddb82eb7e8",
   "metadata": {},
   "source": [
    "<a name=\"LanguageModal\"></a>\n",
    "### 2.4 Language Model\n",
    "For the language model, we simply incorporated the openAI API, provided the ingredient list to the model, and asked it to give it a score of 0 to 10 where 0 means the healthiest and 10 means the least health. \n",
    "After that, we take the average score of the language model’s score and the ML model’s score to serve as the final score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ab4d122f-b47b-4af0-960e-71b828ebab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self):\n",
    "        # Prompt the user for the OpenAI API key or you can change the input by your OpenAI API key\n",
    "        self.OPEN_AI_ACCESS_KEY = input(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "    def language_model_ingredients(self, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Evaluates the healthiness of food based on its ingredient list using OpenAI's language model.\n",
    "        \n",
    "        Parameters:\n",
    "        - file_name (str): The file name containing the ingredient list.\n",
    "        \n",
    "        Returns:\n",
    "        - str: The language model's healthiness rating of the food.\n",
    "        \"\"\"\n",
    "        api_key = self.OPEN_AI_ACCESS_KEY\n",
    "        ingredients = read_text(file_name)  # Ensure the read_text function is defined or import it if it's from another module\n",
    "        prompt = f\"Rate the healthiness of the food on a scale of 1 to 10 based on its nutritional content provided (1 is most healthy and 10 is most unhealthy), give me only the numeric answer: {ingredients}.\"\n",
    "        print(prompt)\n",
    "        \n",
    "        \n",
    "        client = OpenAI(\n",
    "            api_key=self.OPEN_AI_ACCESS_KEY,\n",
    "        )\n",
    "        \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            # Adjust to the model you prefer\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            max_tokens=10,\n",
    "        )\n",
    "\n",
    "        answer = chat_completion.choices[0].message.content\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2bb0ffca-0610-4745-8206-82746f5a8220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:  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\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate the healthiness of the food on a scale of 1 to 10 based on its nutritional content provided (1 is most healthy and 10 is most unhealthy), give me only the numeric answer: ['Nutrition Facts]', 'servings per container', 'Serving size', '1/2 cup (208g)', 'Amount per serving', 'Calories', '240', '% Daily Value*', 'Total Fat 4g', '5%', 'Saturated Fat 1.5g', '8%', 'Trans Fat Og', 'Cholesterol Smg', '2%', 'Sodium 430mg', '19%', 'Total Carbohydrate 46g', '17%', 'Dietary Fiber 7g', '25%', 'Total Sugars 4g', 'Includes 2g Added Sugars', '4%', 'Protein', '11g', 'Vitamin D 2mcg', '10%', 'Calcium 260mg', '20%', 'Iron 6mg', '35%', 'Potassium 240mg', 'The % Daily Value (DV) tells you how much a nutrient in', 'serving of food contributes to', 'daily diet: 2,000 calories', 'day is used for general nutrition advice.'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel()\n",
    "file_name_ingredients = \"../INPUTS/foodlabel.png\"\n",
    "model.language_model_ingredients(file_name_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70aed9-adbe-4eeb-9e71-11eb052be724",
   "metadata": {},
   "source": [
    "<a name=\"RecommendationSystemAlgorithm\" ></a>\n",
    "### 2.5 Recommendation System Algorithm\n",
    "To build the recommendation system, we looked at the final overall score of a food. If it is below a lower boundary, we suggest that the food is great. If it is above a higher boundary, we suggest the wood is unhealthy and shouldn’t be consumed and recommend a few new options. If it is downright in the middle, we suggest the food is okay as it is, but a few new options could be considered. \n",
    "It is important to note that in order to avoid recommending a beverage when the user wants a whole meal (aka recommending something completely unrelated to the user’s needs), we separate the foods into different categories, so that when the user checks the nutritional value of a brownie, we can confidently recommend a protein cupcake, which has a much better rating and  is similar to what the user wants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47510b7b-07a8-4a76-a74b-f5e6f84237d3",
   "metadata": {},
   "source": [
    "<a name=\"UserCenteredResearch\"></a>\n",
    "### 2.6 User-Centered Research\n",
    "We conducted a series of evaluation surveys where we shared the survey to students at Georgia Tech and Northeastern University. http://peersurvey.cc.gatech.edu/s/c93c4b2bf03a491c967ae81ae6b62ffb\n",
    " By walking users through the entire process of using our interface, this survey studies the background, satisfaction, and needs of our users. It provides us with insights on what some of the things we did well on are and how to improve in the future. We then performed data analysis on this data.\n",
    "\n",
    "#### Gender\n",
    "<img src=\"../images/gender.png\" alt=\"Gender\" height=300 />\n",
    "\n",
    "#### Ages\n",
    "<img src=\"../images/ages.png\" alt=\"Ages\" height=300/>\n",
    "\n",
    "#### Fitness Goal\n",
    "<img src=\"../images/image1.png\" alt=\"Fitness Goal\" height=300 />\n",
    "\n",
    "#### Fitness Goal Among Gender\n",
    "<img src=\"../images/image2.png\" alt=\"Fitness Goal Among Gender\" height=300 />\n",
    "\n",
    "#### Fitness Goal Among Ages\n",
    "<img src=\"../images/image3.png\" alt=\"Fitness Goal Among Ages\" height=300 />\n",
    "\n",
    "#### Fitness Goal Among Fitness Levels\n",
    "<img src=\"../images/image4.png\" alt=\"Fitness Goal Among Fitness Levels\" height=300 />\n",
    "\n",
    "#### Improvement in Confidence of Making wise food choice in % among Fitness Levels\n",
    "<img src=\"../images/image5.png\" alt=\"Confidence Level\" height=300 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33214a7c-b79d-4030-b0e6-abdadaa8083a",
   "metadata": {},
   "source": [
    "<a name=\"Streamlit\"></a>\n",
    "### 2.7 Streamlit Dashboard\n",
    "We used streamlit as a visualization tool to create a dashboard that walks the user down the process. Please run the code to see for yourself.\n",
    "\n",
    "\n",
    "---\n",
    "####  Running the Streamlit App\n",
    "\n",
    "To interact with the Streamlit application associated with this notebook, please follow these steps:\n",
    "\n",
    "1. **Open a Terminal**: Open a new terminal window on your computer. This can usually be done through your operating system's main menu or search function.\n",
    "\n",
    "2. **Activate Your Environment** (if applicable): If you're using a virtual environment for your project (recommended), activate it by running the appropriate command for your virtual environment. For example, if you're using `conda`, you might use:\n",
    "   ```bash\n",
    "   conda activate your_env_name\n",
    "   ```\n",
    "   Or, if you're using `venv`, you might use:\n",
    "   ```bash\n",
    "   source your_env_name/bin/activate\n",
    "   ```\n",
    "   Replace `your_env_name` with the name of your virtual environment.\n",
    "\n",
    "3. **Navigate to Your Project Directory**: Use the `cd` command to navigate to the directory containing your Streamlit script. For example:\n",
    "   ```bash\n",
    "   cd path/to/your/project/streamlit\n",
    "   ```\n",
    "   Replace `path/to/your/project/streamlit` with the actual path to your project directory.\n",
    "\n",
    "4. **Run the Streamlit App**: Execute the Streamlit app by running:\n",
    "   ```bash\n",
    "   streamlit run streamlit.py\n",
    "   ```\n",
    "   Replace `streamlit.py` with the name of your Streamlit script file.\n",
    "\n",
    "5. **View the App**: After running the above command, Streamlit will start the server and open your default web browser to display the app. If the browser does not open automatically, Streamlit will provide a URL in the terminal that you can copy and paste into your browser to view the app.\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "- **Dependencies**: Ensure all dependencies required by your Streamlit app are installed in your environment. This may include libraries like `pandas`, `numpy`, `PIL`, and any others your app uses.\n",
    "- **Troubleshooting**: If you encounter any issues running the app, check that you're in the correct directory, your virtual environment is activated, and all necessary Python packages are installed.\n",
    "- **Feedback and Iteration**: Encourage users to provide feedback on the app. If you're using this in an educational or collaborative setting, this can be a great way to iterate on and improve the app.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c08d9-9a1e-464c-809d-c53bef05b9c1",
   "metadata": {},
   "source": [
    "<a name=\"Conclusion\"></a>\n",
    "## 3. Conclusion\n",
    "Next steps:\n",
    "We plan to improve on OCR so that more accurate text extraction can be done in real-life image captioning.\n",
    "We plan to work on details of our model, including a bigger dataset, more features, the consideration of one’s physical and dietary needs, and other factors to both lower MSE of our model and improve user experiences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
